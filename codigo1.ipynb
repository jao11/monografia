{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdeCh/685gdYZ2RySESkGI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jao11/monografia/blob/main/codigo1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Codigo criado para a criação dos x_train, y_train, x_test e y_test."
      ],
      "metadata": {
        "id": "InNKarK0SNrF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bibliotecas:"
      ],
      "metadata": {
        "id": "HFgB1dlhw-vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Montando o drive:"
      ],
      "metadata": {
        "id": "69lVQfq-xUwP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ8Cucp5w1iU",
        "outputId": "147959bb-9758-43ea-abff-810a5bea2ad9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalando bibliotecas e fazendo imports necessarios:"
      ],
      "metadata": {
        "id": "uEqCPdRYxwg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# !python3 -m pip install mplhep coffea\n",
        "# import mplhep as hep\n",
        "# import coffea.hist as hist\n",
        "# plt.style.use(hep.style.ROOT)"
      ],
      "metadata": {
        "id": "_7XOxD6kxeF7"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dados:"
      ],
      "metadata": {
        "id": "WY2OSrqN1DXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constantes facilitadoras e colunas::"
      ],
      "metadata": {
        "id": "r_uLjPFt1KjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raiz_s = 13000 # GeV - consante que representa a energia na colisão dos prótons\n",
        "teste_size = 0.3 # ??\n",
        "PATH = '/content/drive/MyDrive/material_uerj/Monografia/minha_monografia/codigo/'\n",
        "PATH2 = 'refazendo/treino/'\n",
        "\n",
        "coluna = [b'W_Mass', b'W_pt_lep', b'dPhi_Whad_Wlep', b'dPhi_jatos_MET', b'jetAK8_pt', b'jetAK8_eta', b'jetAK8_prunedMass', b'jetAK8_tau21', \n",
        "            b'METPt', b'muon_pt', b'muon_eta', b'ExtraTracks', b'PUWeight', b'W_rapidity', b'btag', b'xi1', b'xi2', b'ismultirp1', b'ismultirp2',\n",
        "            b'Norm', b'weight', b'Mpps', b'Ypps', b'Mww/Mpps', b'Ypps-Yww']\n",
        "\n",
        "coluna_LGBM = [b'W_Mass', b'W_pt_lep', b'dPhi_Whad_Wlep', b'dPhi_jatos_MET', b'jetAK8_pt', b'jetAK8_eta', b'jetAK8_prunedMass', b'jetAK8_tau21', \n",
        "            b'METPt', b'muon_pt', b'muon_eta', b'ExtraTracks', b'W_rapidity', b'xi1', b'xi2',  b'Mpps', b'Ypps', b'Mww/Mpps', b'Ypps-Yww']"
      ],
      "metadata": {
        "id": "4TvrgvJDzm9p"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções:\n"
      ],
      "metadata": {
        "id": "ZvsVWjbi2MfV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Função para abertura dos arquivos:"
      ],
      "metadata": {
        "id": "u_yu-nhp4tg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def open_file( file ): # abertura dos arquivos de sinal.\n",
        "  df = None\n",
        "  with h5py.File( file, 'r') as f: \n",
        "    # O data set é um arquivo que pode conter centenas ou até milhares de dados sobre um determinado assunto.\n",
        "    dset_columns = f['columns'] \n",
        "    dset_dados = f['dados']\n",
        "    # print('\\n colunas  -->', np.array( dset_columns),'\\n')\n",
        "    df = pd.DataFrame( np.array(dset_dados), columns = np.array(dset_columns))\n",
        "    df[b'Mpps'] = raiz_s * ( np.sqrt( df[b'xi1'] * df[b'xi2'] ) ) # Massa  PPS ou Massa perdida.\n",
        "    df[b'Ypps'] = 1/2 * np.log( df[b'xi1'] / df[b'xi2'] ) # Pseudorapidez medida no sistema central.\n",
        "    df[b'Mww/Mpps'] = df[b'W_Mass'] / df[b'Mpps']\n",
        "    df[b'Ypps-Yww'] = df[b'Ypps'] - df[b'W_rapidity']\n",
        "\n",
        "    df_cut = (df[b'muon_pt'] > 53) & (df[b'xi1'] > 0.04) & (df[b'xi2'] > 0.04) & (df[b'xi1'] < 0.111) & (df[b'xi2'] < 0.138) & (df[b'muon_eta'] < 2.4) & (df[b'jetAK8_pt'] > 200) & (df[b'jetAK8_eta'] < 2.4) & (df[b'METPt'] > 40) & (df[b'W_pt_lep'] > 200)\n",
        "    dset = df[df_cut]\n",
        "    return dset\n",
        "\n",
        "def open_file_2( file ): # abertura dos arquivos de Background.\n",
        "  df = None\n",
        "  with h5py.File( file , 'r' ) as f:\n",
        "    # O data set é um arquivo que pode conter centenas ou até milhares de dados sobre um determinado assunto.\n",
        "    dset_columns = f['columns']\n",
        "    dset_dados = f['dados']\n",
        "    # print( '\\n colunas --> ', np.array( dset_columns ),'\\n' )\n",
        "    df = pd.DataFrame( np.array(dset_dados), columns = np.array( dset_columns))\n",
        "    df[b'Mpps'] = raiz_s * ( np.sqrt( df[b'xi1'] * df[b'xi2'] ) ) # Massa  PPS ou Massa perdida.\n",
        "    df[b'Ypps'] = 1/2 * np.log( df[b'xi1'] / df[b'xi2'] ) # Pseudorapidez medida no sistema central.\n",
        "    df[b'Mww/Mpps'] = df[b'W_Mass'] / df[b'Mpps']\n",
        "    df[b'Ypps-Yww'] = df[b'Ypps'] - df[b'W_rapidity']\n",
        "\n",
        "    df_cut = (df[b'muon_pt'] > 53) & (df[b'xi1'] > 0.04) & (df[b'xi2'] > 0.04) & (df[b'xi1'] < 0.111) & (df[b'xi2'] < 0.138) & (df[b'muon_eta'] < 2.4) & (df[b'jetAK8_pt'] > 200) & (df[b'jet_eta'] < 2.4) & (df[b'METPt'] > 40) & (df[b'W_pt_lep'] > 200)\n",
        "    dset = df[df_cut]\n",
        "    return dset\n"
      ],
      "metadata": {
        "id": "Z2RffNor2OxW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Abrindo os arquivos:"
      ],
      "metadata": {
        "id": "e5DVThoeS5nW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Abrindo e corrigindo os arquivos de background.\n",
        "\n",
        "TTbar = pd.DataFrame( np.array(open_file_2(PATH + 'dados/background/DataSet_TTbar.h5')), columns = coluna)\n",
        "DrellYan = pd.DataFrame( np.array(open_file_2(PATH + 'dados/background/DataSet_multiRP_DrellYan.h5')),columns = coluna)\n",
        "QCD = pd.DataFrame( np.array(open_file_2(PATH + 'dados/background/DataSet_multiRP_QCD.h5')),columns = coluna)\n",
        "sing_top = pd.DataFrame( np.array(open_file_2(PATH + 'dados/background/DataSet_multiRP_single_top.h5')),columns = coluna)\n",
        "VV_inclusivo = pd.DataFrame( np.array(open_file_2(PATH + 'dados/background/DataSet_multiRP_VV_inclusivo.h5')),columns = coluna)\n",
        "W_jets = pd.DataFrame( np.array(open_file_2(PATH + 'dados/background/DataSet_multiRP_WJets.h5')),columns = coluna)\n",
        "\n",
        "# Abrindo os arquivos de sinal.\n",
        "#     alhpac = 1 ao 4 \n",
        "#     alpha0 = 5 ao 8 \n",
        "\n",
        "anomalo_1 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO1_multiRP.h5')\n",
        "anomalo_2 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO2_multiRP.h5')\n",
        "anomalo_3 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO3_multiRP.h5')\n",
        "anomalo_4 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO4_multiRP.h5')\n",
        "anomalo_5 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO5_multiRP.h5')\n",
        "anomalo_6 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO6_multiRP.h5')\n",
        "anomalo_7 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO7_multiRP.h5')\n",
        "anomalo_8 = open_file(PATH + 'dados/anomalos+sm/output-DataSet_ANOMALO8_multiRP.h5')\n",
        "\n",
        "# Abrindo o arquivo de SM.\n",
        "\n",
        "SM = open_file(PATH + 'dados/anomalos+sm/output-SM_multiRP.h5')\n"
      ],
      "metadata": {
        "id": "bbCBly4kS41p"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Colocando as colunas de rótulo nos arquivos:"
      ],
      "metadata": {
        "id": "hGnl04vGao41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sinais anomalos.\n",
        "\n",
        "anomalo_1['label'] = 1\n",
        "anomalo_2['label'] = 1\n",
        "anomalo_3['label'] = 1\n",
        "anomalo_4['label'] = 1\n",
        "anomalo_5['label'] = 1\n",
        "anomalo_6['label'] = 1\n",
        "anomalo_7['label'] = 1\n",
        "anomalo_8['label'] = 1\n",
        "\n",
        "# Sinal do SM.\n",
        "\n",
        "SM['label'] = 1\n",
        "\n",
        "# Background.\n",
        "\n",
        "TTbar['label'] = 0\n",
        "DrellYan['label'] = 0\n",
        "QCD['label'] = 0\n",
        "sing_top['label'] = 0\n",
        "VV_inclusivo['label'] = 0\n",
        "W_jets['label'] = 0\n"
      ],
      "metadata": {
        "id": "u5cXsQeua9GK"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Concatenando:\n",
        "\n",
        " *Ou* seja juntando as informações de background com os anomalos e o SM. E por fim embaralhando os dados concatenados."
      ],
      "metadata": {
        "id": "DrTlOtwzcWQr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_anomalo_1 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_1, SM], axis=0))\n",
        "x_anomalo_2 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_2, SM], axis=0))\n",
        "x_anomalo_3 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_3, SM], axis=0))\n",
        "x_anomalo_4 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_4, SM], axis=0))\n",
        "x_anomalo_5 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_5, SM], axis=0))\n",
        "x_anomalo_6 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_6, SM], axis=0))\n",
        "x_anomalo_7 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_7, SM], axis=0))\n",
        "x_anomalo_8 = shuffle(pd.concat([TTbar, DrellYan, QCD, VV_inclusivo, W_jets, sing_top, anomalo_8, SM], axis=0))"
      ],
      "metadata": {
        "id": "FkA8Q8M0c6p_"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rotulando amostras e criando os conjuntos x e y"
      ],
      "metadata": {
        "id": "9I7T-XEWeQ1m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto x:"
      ],
      "metadata": {
        "id": "xF6s2uKBfuVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --> Criando os x_train e os x_test:\n",
        "\n",
        "x_anomalo_1_train, x_anomalo_1_test = train_test_split( x_anomalo_1, test_size = teste_size, random_state = 42, stratify = x_anomalo_1.label )\n",
        "x_anomalo_2_train, x_anomalo_2_test = train_test_split( x_anomalo_2, test_size = teste_size, random_state = 42, stratify = x_anomalo_2.label )\n",
        "x_anomalo_3_train, x_anomalo_3_test = train_test_split( x_anomalo_3, test_size = teste_size, random_state = 42, stratify = x_anomalo_3.label )\n",
        "x_anomalo_4_train, x_anomalo_4_test = train_test_split( x_anomalo_4, test_size = teste_size, random_state = 42, stratify = x_anomalo_4.label )\n",
        "x_anomalo_5_train, x_anomalo_5_test = train_test_split( x_anomalo_5, test_size = teste_size, random_state = 42, stratify = x_anomalo_5.label )\n",
        "x_anomalo_6_train, x_anomalo_6_test = train_test_split( x_anomalo_6, test_size = teste_size, random_state = 42, stratify = x_anomalo_6.label )\n",
        "x_anomalo_7_train, x_anomalo_7_test = train_test_split( x_anomalo_7, test_size = teste_size, random_state = 42, stratify = x_anomalo_7.label )\n",
        "x_anomalo_8_train, x_anomalo_8_test = train_test_split( x_anomalo_8, test_size = teste_size, random_state = 42, stratify = x_anomalo_8.label )\n",
        "\n",
        "# --> Determinando os Pesos:\n",
        "\n",
        "weight_anomalo_1 = x_anomalo_1_test[b'weight']\n",
        "weight_anomalo_2 = x_anomalo_2_test[b'weight']\n",
        "weight_anomalo_3 = x_anomalo_3_test[b'weight']\n",
        "weight_anomalo_4 = x_anomalo_4_test[b'weight']\n",
        "weight_anomalo_5 = x_anomalo_5_test[b'weight']\n",
        "weight_anomalo_6 = x_anomalo_6_test[b'weight']\n",
        "weight_anomalo_7 = x_anomalo_7_test[b'weight']\n",
        "weight_anomalo_8 = x_anomalo_8_test[b'weight']\n"
      ],
      "metadata": {
        "id": "_bKyAAq7DQvi"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "x_anomalo_1 = x_anomalo_1[coluna_LGBM]\n",
        "x_anomalo_1_train = x_anomalo_1_train[coluna_LGBM]\n",
        "x_anomalo_1_test  = x_anomalo_1_test[coluna_LGBM]\n",
        "\n",
        "x_anomalo_2 = x_anomalo_2[coluna_LGBM]\n",
        "x_anomalo_2_train = x_anomalo_2_train[coluna_LGBM]\n",
        "x_anomalo_2_test  = x_anomalo_2_test[coluna_LGBM]\n",
        "\n",
        "x_anomalo_3 = x_anomalo_3[coluna_LGBM]\n",
        "x_anomalo_3_train = x_anomalo_3_train[coluna_LGBM]\n",
        "x_anomalo_3_test  = x_anomalo_3_test[coluna_LGBM]\n",
        "\n",
        "x_anomalo_4 = x_anomalo_4[coluna_LGBM]\n",
        "x_anomalo_4_train = x_anomalo_4_train[coluna_LGBM]\n",
        "x_anomalo_4_test  = x_anomalo_4_test[coluna_LGBM]\n",
        "\n",
        "x_anomalo_5 = x_anomalo_5[coluna_LGBM]\n",
        "x_anomalo_5_train = x_anomalo_5_train[coluna_LGBM]\n",
        "x_anomalo_5_test  = x_anomalo_5_test[coluna_LGBM]\n",
        "\n",
        "x_anomalo_6 = x_anomalo_6[coluna_LGBM]\n",
        "x_anomalo_6_train = x_anomalo_6_train[coluna_LGBM]\n",
        "x_anomalo_6_test  = x_anomalo_6_test[coluna_LGBM]\n",
        "\n",
        "x_anomalo_7 = x_anomalo_7[coluna_LGBM]\n",
        "x_anomalo_7_train = x_anomalo_7_train[coluna_LGBM]\n",
        "x_anomalo_7_test  = x_anomalo_7_test[coluna_LGBM]\n",
        "\n",
        "x_anomalo_8 = x_anomalo_8[coluna_LGBM]\n",
        "x_anomalo_8_train = x_anomalo_8_train[coluna_LGBM]\n",
        "x_anomalo_8_test  = x_anomalo_8_test[coluna_LGBM]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BduoQYJWmTBJ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conjunto y:"
      ],
      "metadata": {
        "id": "dXvZON6Hk447"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --> Criando os y_train e os y_test:\n",
        "\n",
        "y_anomalo_1= x_anomalo_1['label']\n",
        "y_anomalo_2= x_anomalo_2['label']\n",
        "y_anomalo_3= x_anomalo_3['label']\n",
        "y_anomalo_4= x_anomalo_4['label']\n",
        "y_anomalo_5= x_anomalo_5['label']\n",
        "y_anomalo_6= x_anomalo_6['label']\n",
        "y_anomalo_7= x_anomalo_7['label']\n",
        "y_anomalo_8= x_anomalo_8['label']\n",
        "\n",
        "\n",
        "y_anomalo_1_train = x_anomalo_1_train['label']\n",
        "y_anomalo_1_test  = x_anomalo_1_test['label']\n",
        "\n",
        "y_anomalo_2_train = x_anomalo_2_train['label']\n",
        "y_anomalo_2_test  = x_anomalo_2_test['label']\n",
        "\n",
        "y_anomalo_3_train = x_anomalo_3_train['label']\n",
        "y_anomalo_3_test  = x_anomalo_3_test['label']\n",
        "\n",
        "y_anomalo_4_train = x_anomalo_4_train['label']\n",
        "y_anomalo_4_test  = x_anomalo_4_test['label']\n",
        "\n",
        "y_anomalo_5_train = x_anomalo_5_train['label']\n",
        "y_anomalo_5_test  = x_anomalo_5_test['label']\n",
        "\n",
        "y_anomalo_6_train = x_anomalo_6_train['label']\n",
        "y_anomalo_6_test  = x_anomalo_6_test['label']\n",
        "\n",
        "y_anomalo_7_train = x_anomalo_7_train['label']\n",
        "y_anomalo_7_test  = x_anomalo_7_test['label']\n",
        "\n",
        "y_anomalo_8_train = x_anomalo_8_train['label']\n",
        "y_anomalo_8_test  = x_anomalo_8_test['label']\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eEgMlZQMPetF"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Salvando os x_train, x_test, y_train e y_test:"
      ],
      "metadata": {
        "id": "DDi9ZIDen41u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_1_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_1_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_1_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_1_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_1_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_1_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_1_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_1_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_1.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_1)\n",
        " \n",
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_2_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_2_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_2_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_2_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_2_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_2_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_2_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_2_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_2.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_2)\n",
        "\n",
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_3_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_3_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_3_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_3_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_3_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_3_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_3_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_3_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_3.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_3)\n",
        " \n",
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_4_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_4_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_4_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_4_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_4_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_4_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_4_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_4_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_4.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_4)\n",
        "\n",
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_5_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_5_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_5_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_5_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_5_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_5_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_5_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_5_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_5.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_5)\n",
        " \n",
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_6_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_6_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_6_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_6_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_6_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_6_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_6_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_6_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_6.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_6)\n",
        "\n",
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_7_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_7_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_7_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_7_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_7_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_7_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_7_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_7_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_7.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_7)\n",
        " \n",
        "with h5py.File(PATH + PATH2 + 'x_train/x_anomalo_8_train.h5', 'w') as f:  \n",
        "  dset = f.create_dataset(\"treino\", data = x_anomalo_8_train)\n",
        "  with h5py.File(PATH + PATH2 + 'x_test/x_anomalo_8_test.h5', 'w') as f:  \n",
        "    dset = f.create_dataset(\"treino\", data = x_anomalo_8_test)\n",
        "    with h5py.File(PATH + PATH2 + 'y_train/y_anomalo_8_train.h5', 'w') as f:  \n",
        "      dset = f.create_dataset(\"treino\", data = y_anomalo_8_train)\n",
        "      with h5py.File(PATH + PATH2 + 'y_test/y_anomalo_8_test.h5', 'w') as f:  \n",
        "        dset = f.create_dataset(\"treino\", data = y_anomalo_8_test)\n",
        "        with h5py.File(PATH + PATH2 + 'weight_anomalo/weight_anomalo_8.h5', 'w') as f:  \n",
        "          dset = f.create_dataset(\"treino\", data = weight_anomalo_8)\n",
        "\n"
      ],
      "metadata": {
        "id": "XHpzX-fGpqZZ"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XqUvrGqmtJKl"
      },
      "execution_count": 94,
      "outputs": []
    }
  ]
}